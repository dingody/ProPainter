#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ProPainter æ˜¾å­˜é—®é¢˜å¿«é€Ÿè§£å†³æ–¹æ¡ˆ
ä¸“ä¸ºä½ çš„720pè§†é¢‘æ˜¾å­˜ä¸è¶³é—®é¢˜è®¾è®¡
"""

import os
import torch
import gc
import subprocess
import sys

def immediate_fix():
    """ç«‹å³è§£å†³æ˜¾å­˜é—®é¢˜çš„æ–¹æ¡ˆ"""
    
    print("ğŸš¨ æ˜¾å­˜ä¸è¶³ç´§æ€¥ä¿®å¤æ–¹æ¡ˆ")
    print("=" * 60)
    
    # 1. è®¾ç½®ç¯å¢ƒå˜é‡
    print("1ï¸âƒ£ è®¾ç½®PyTorchå†…å­˜ç®¡ç†...")
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    
    # 2. å¼ºåˆ¶æ¸…ç†å†…å­˜
    print("2ï¸âƒ£ å¼ºåˆ¶æ¸…ç†GPUå†…å­˜...")
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
        gc.collect()
        
        # æ˜¾ç¤ºæ¸…ç†åçš„å†…å­˜çŠ¶æ€
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        allocated = torch.cuda.memory_allocated(0) / 1024**3  
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        free = total - reserved
        
        print(f"   æ€»å†…å­˜: {total:.2f}GB")
        print(f"   å·²åˆ†é…: {allocated:.2f}GB")
        print(f"   å·²ä¿ç•™: {reserved:.2f}GB") 
        print(f"   å¯ç”¨: {free:.2f}GB")
        
        if free < 3.0:
            print("âš ï¸  å¯ç”¨å†…å­˜ä¸è¶³3GBï¼Œå»ºè®®é‡å¯Runtime")
    
    # 3. æä¾›å…·ä½“çš„è§£å†³å‘½ä»¤
    print("\n3ï¸âƒ£ é’ˆå¯¹ä½ çš„é—®é¢˜çš„å…·ä½“è§£å†³æ–¹æ¡ˆ:")
    print("-" * 40)
    
    solutions = [
        {
            "level": "ğŸŸ¢ æ–¹æ¡ˆ1: è¶…ä½å†…å­˜æ¨¡å¼ (å¼ºçƒˆæ¨è)",
            "resolution": "480x270", 
            "memory": "çº¦2GB",
            "cmd": "python inference_propainter.py -i /content/input_fixed.mp4 -m /content/input_fixed_mask --fp16 --width 480 --height 270 --subvideo_length 6 --neighbor_length 4 --ref_stride 20"
        },
        {
            "level": "ğŸŸ¡ æ–¹æ¡ˆ2: ä½å†…å­˜æ¨¡å¼",
            "resolution": "640x360",
            "memory": "çº¦3GB", 
            "cmd": "python inference_propainter.py -i /content/input_fixed.mp4 -m /content/input_fixed_mask --fp16 --width 640 --height 360 --subvideo_length 8 --neighbor_length 5 --ref_stride 15"
        },
        {
            "level": "ğŸŸ  æ–¹æ¡ˆ3: ä¸­ç­‰å†…å­˜æ¨¡å¼",
            "resolution": "854x480",
            "memory": "çº¦4GB",
            "cmd": "python inference_propainter.py -i /content/input_fixed.mp4 -m /content/input_fixed_mask --fp16 --width 854 --height 480 --subvideo_length 10 --neighbor_length 6"
        }
    ]
    
    for solution in solutions:
        print(f"\n{solution['level']}")
        print(f"   åˆ†è¾¨ç‡: {solution['resolution']}")
        print(f"   é¢„ä¼°å†…å­˜: {solution['memory']}")
        print(f"   å‘½ä»¤: {solution['cmd']}")
    
    return solutions

def create_emergency_script():
    """åˆ›å»ºç´§æ€¥å¤„ç†è„šæœ¬"""
    
    script_content = '''#!/usr/bin/env python3
import os
import torch
import gc
import subprocess

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# æ¸…ç†å†…å­˜
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()
    gc.collect()

print("ğŸ§¹ å†…å­˜å·²æ¸…ç†")

# å°è¯•ä¸åŒçš„è®¾ç½®ï¼Œä»æœ€ä¿å®ˆå¼€å§‹
configs = [
    {
        "name": "è¶…ä¿å®ˆæ¨¡å¼",
        "args": ["--width", "480", "--height", "270", "--subvideo_length", "6", "--neighbor_length", "4"]
    },
    {
        "name": "ä¿å®ˆæ¨¡å¼", 
        "args": ["--width", "640", "--height", "360", "--subvideo_length", "8", "--neighbor_length", "5"]
    },
    {
        "name": "ä¸­ç­‰æ¨¡å¼",
        "args": ["--width", "854", "--height", "480", "--subvideo_length", "10", "--neighbor_length", "6"]
    }
]

for config in configs:
    print(f"\\nğŸ¯ å°è¯• {config['name']}...")
    
    cmd = [
        "python", "inference_propainter.py",
        "-i", "/content/input_fixed.mp4",
        "-m", "/content/input_fixed_mask", 
        "--fp16"
    ] + config["args"]
    
    try:
        result = subprocess.run(cmd, check=True, capture_output=False)
        print(f"âœ… {config['name']} æˆåŠŸå®Œæˆï¼")
        break
    except subprocess.CalledProcessError as e:
        print(f"âŒ {config['name']} å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")
        # æ¸…ç†å†…å­˜åç»§ç»­
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
        continue
else:
    print("âŒ æ‰€æœ‰æ¨¡å¼éƒ½å¤±è´¥äº†ï¼Œè¯·è€ƒè™‘é‡å¯Runtimeæˆ–åˆ†æ®µå¤„ç†")
'''
    
    with open('emergency_fix.py', 'w') as f:
        f.write(script_content)
    
    print("âœ… å·²åˆ›å»ºç´§æ€¥ä¿®å¤è„šæœ¬: emergency_fix.py")

def restart_runtime_guide():
    """é‡å¯RuntimeæŒ‡å—"""
    
    print("\nğŸ”„ å¦‚æœä¸Šè¿°æ–¹æ¡ˆéƒ½å¤±è´¥ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤é‡å¯Runtime:")
    print("-" * 50)
    print("1. åœ¨Colabèœå•æ ç‚¹å‡» 'ä»£ç æ‰§è¡Œç¨‹åº' â†’ 'é‡æ–°å¯åŠ¨ä¼šè¯'")
    print("2. æˆ–è€…ä½¿ç”¨å¿«æ·é”®: Ctrl+M .")
    print("3. é‡å¯åé‡æ–°è¿è¡Œåˆå§‹åŒ–è„šæœ¬")
    print("4. ç„¶åä½¿ç”¨æ–¹æ¡ˆ1çš„è¶…ä½å†…å­˜æ¨¡å¼")
    
    print("\nğŸ’¡ é‡å¯åçš„æ¨èæµç¨‹:")
    print("# é‡æ–°åˆå§‹åŒ–")
    print("!python init_colab.py")
    print()
    print("# ä½¿ç”¨è¶…ä¿å®ˆè®¾ç½®")
    print("!python inference_propainter.py -i /content/input_fixed.mp4 -m /content/input_fixed_mask --fp16 --width 480 --height 270 --subvideo_length 6 --neighbor_length 4")

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("ğŸš‘ ProPainter æ˜¾å­˜é—®é¢˜æ€¥æ•‘åŒ…")
    print("=" * 60)
    
    # ç«‹å³æ‰§è¡Œä¿®å¤
    solutions = immediate_fix()
    
    # åˆ›å»ºç´§æ€¥è„šæœ¬
    print("\n4ï¸âƒ£ åˆ›å»ºè‡ªåŠ¨å¤„ç†è„šæœ¬...")
    create_emergency_script()
    
    # æä¾›é‡å¯æŒ‡å—
    restart_runtime_guide()
    
    print("\n" + "=" * 60)
    print("ğŸ¯ ç«‹å³è¡ŒåŠ¨æ–¹æ¡ˆ:")
    print("1ï¸âƒ£ å¤åˆ¶å¹¶è¿è¡Œæ–¹æ¡ˆ1çš„å‘½ä»¤ï¼ˆè¶…ä½å†…å­˜æ¨¡å¼ï¼‰")
    print("2ï¸âƒ£ å¦‚æœå¤±è´¥ï¼Œè¿è¡Œ: !python emergency_fix.py")
    print("3ï¸âƒ£ å¦‚æœè¿˜å¤±è´¥ï¼Œé‡å¯Runtimeåé‡è¯•")
    print("4ï¸âƒ£ æœ€åé€‰æ‹©ï¼šè€ƒè™‘å°†è§†é¢‘åˆ†æ®µå¤„ç†")
    
    print(f"\nğŸ”¥ æœ€æ¨èçš„å‘½ä»¤ï¼ˆç›´æ¥å¤åˆ¶è¿è¡Œï¼‰:")
    print("!" + solutions[0]["cmd"])

if __name__ == "__main__":
    main()